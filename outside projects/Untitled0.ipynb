{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rpLUFj6gVx5otMtm9UqufBkz-vjtAswu","authorship_tag":"ABX9TyNaOufzgBTP5mNeAH1OTdP7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ZA0CiPkyaxp9","executionInfo":{"status":"ok","timestamp":1679406452737,"user_tz":-330,"elapsed":1563,"user":{"displayName":"pkmushthaq","userId":"01383379457071146275"}}},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import urllib\n","\n","\n","\n","\n","# import requests\n","# from bs4 import BeautifulSoup\n","# import urllib\n","\n","# # Define search query\n","# search_term = \"dogs\"\n","\n","# # Define the number of images to collect\n","# num_images = 1000\n","\n","# # Define the URL to scrape\n","# url = f\"https://www.google.com/search?q={search_term}&rlz=1C1GCEU_enUS832US832&source=lnms&tbm=isch\"\n","\n","# # Define the user agent (optional)\n","# headers = {\n","#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n","\n","# # Make a request to the URL\n","# response = requests.get(url, headers=headers)\n","\n","# # Parse the HTML content of the page\n","# soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# # Find all image tags\n","# img_tags = soup.find_all(\"img\")\n","\n","# # Collect image URLs\n","# urls = []\n","# for img in img_tags:\n","#     try:\n","#         # Get the URL of the image\n","#         url = img[\"data-src\"]\n","#         # Append the URL to the list\n","#         urls.append(url)\n","#     except KeyError:\n","#         continue\n","\n","# # Download images\n","# for i in range(num_images):\n","#     try:\n","#         # Open the URL image, set stream to True, and retrieve the content\n","#         response = requests.get(urls[i], stream=True)\n","#         # Set the path and filename to save the image\n","#         path = f\"images/{search_term}{i}.jpg\"\n","#         # Open the file and write the content of the image to the file\n","#         with open(path, 'wb') as f:\n","#             f.write(response.content)\n","#     except:\n","#         continue\n","\n","# print(f\"{num_images} images of {search_term} have been downloaded to the images folder.\")\n"]},{"cell_type":"code","source":["search_term = \"rebar spalling\"\n","\n","num_images = 10\n","\n","url = f'https://www.google.com/search?q={search_term}&sxsrf=AJOqlzV2aYT622SoRLTBXzRxhD94xuHmCA:1679386107901&source=lnms&tbm=isch&sa=X&ved=2ahUKEwi4ppvjyOz9AhXncGwGHXhlDtEQ_AUoAXoECAIQAw&biw=1366&bih=636&dpr=1'\n","\n","\n","response = requests.get(url)\n","\n","soup = BeautifulSoup(response.content, 'html.parser')\n","\n","img_tags = soup.find_all('img')\n","\n","urls = []\n","\n","for img in img_tags:\n","    try:\n","        url = img['src']\n","        urls.append(url)\n","    except KeyError:\n","        continue\n","\n","for i in range(num_images):\n","    try:\n","        response = requests.get(urls[i],stream=True)\n","\n","        path = f'/content/drive/MyDrive/DS/PYTHON/Web_scraping/scrapping/{search_term}{i}.jpg'\n","\n","        with open(path, 'wb') as f:\n","            f.write(response.content)\n","            print(f'---> print image no{i}')\n","\n","\n","    except:\n","        continue\n","\n","        \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L9n5FKe1a7YG","executionInfo":{"status":"ok","timestamp":1679406627502,"user_tz":-330,"elapsed":2285,"user":{"displayName":"pkmushthaq","userId":"01383379457071146275"}},"outputId":"132bc2e0-f104-44f2-f753-14e34140fc68"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["---> 1\n","---> 2\n","---> 3\n","---> 4\n","---> 5\n","---> 6\n","---> 7\n","---> 8\n","---> 9\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JDNyB_X4bheG"},"execution_count":null,"outputs":[]}]}